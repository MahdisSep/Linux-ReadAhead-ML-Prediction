# -*- coding: utf-8 -*-
"""OSLabProject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-UZQNS7IFPIZtQFnYK0DfllIRXFmQh_B
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.manifold import TSNE
from sklearn.model_selection import StratifiedKFold, cross_val_score, GridSearchCV
from sklearn.tree import DecisionTreeClassifier,plot_tree
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score,confusion_matrix,precision_score, recall_score,make_scorer,f1_score

"""Let's create a pandas dataframe for each class and then combine them together"""

df_readrandom = pd.read_csv('readrandom.txt', delimiter=',')
df_readrandom['label'] = 'readrandom'

df_readrandomwriterandom = pd.read_csv('readrandomwriterandom.txt', delimiter=',')
df_readrandomwriterandom['label'] = 'readrandomwriterandom'

df_readreverse = pd.read_csv('readreverse.txt', delimiter=',')
df_readreverse['label'] = 'readreverse'

df_readseq = pd.read_csv('readseq.txt', delimiter=',')
df_readseq['label'] = 'readseq'

df_readrandom.head()

df_readrandomwriterandom.head()

df_readreverse.head()

df_readseq.head()

df = pd.concat([df_readrandom, df_readrandomwriterandom, df_readreverse, df_readseq], ignore_index=True)

print(df_readrandom.shape)

print(df_readrandomwriterandom.shape)

print(df_readreverse.shape)

print(df_readseq.shape)

df.head()

df.to_csv('data2.csv', index=False)

file_path = 'data2.csv'
df = pd.read_csv(file_path)

df.info()

df.describe()

missing_values = df.isnull().sum()

print("Number of missing values in each column:")
print(missing_values)

df = df.dropna()

df.info()

label_encoder = LabelEncoder()

df['label_encoded'] = label_encoder.fit_transform(df['label'])

df.head()

df = df.drop(columns=['label_encoded'])

df.head()

df_encoded= pd.get_dummies(df, columns=['label'], drop_first=False)

df_encoded.head()

"""Let's normalize our dataset by using Z-score technique"""

numerical_columns = ['Ino', 'State', 'flag', 'transaction', 'time elapsed','culmulative time elapsed']
scaler = StandardScaler()

df_combined_normalized = df_encoded.copy()
df_combined_normalized[numerical_columns] = scaler.fit_transform(df_combined_normalized[numerical_columns])

df_combined_normalized.head()

df_combined_normalized.describe()

X = df_combined_normalized[numerical_columns]

tsne = TSNE(n_components=2, random_state=42)

X_tsne = tsne.fit_transform(X)

df_tsne = pd.DataFrame(X_tsne, columns=['Dimension 1', 'Dimension 2'])
df_tsne['label'] = df['label']

plt.figure(figsize=(10, 8))
sns.scatterplot(x='Dimension 1', y='Dimension 2', hue='label', data=df_tsne, palette='viridis')
plt.title('t-SNE Visualization')
plt.show()

X = df_combined_normalized.drop(columns=['label_readrandom', 'label_readrandomwriterandom', 'label_readreverse', 'label_readseq'])
y = df_combined_normalized[['label_readrandom', 'label_readrandomwriterandom', 'label_readreverse', 'label_readseq']]

y.head()

label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y.idxmax(axis=1))

print(y_encoded)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("Shape of X_train:", X_train.shape)
print("Shape of X_test:", X_test.shape)
print("Shape of y_train:", y_train.shape)
print("Shape of y_test:", y_test.shape)

label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train.idxmax(axis=1))
y_test_encoded = label_encoder.transform(y_test.idxmax(axis=1))

def build_model():
    model = Sequential()
    model.add(Dense(16, activation='relu', input_shape=(X_train.shape[1],)))
    model.add(Dense(32, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(64, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(4, activation='softmax'))

    alpha_schedule = tf.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.0001, decay_steps=10000, decay_rate=0.9)
    optimizer = Adam(learning_rate=alpha_schedule)
    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

    return model

"""Let's train the model using 10-fold cross validation to prevent from overfitting"""

from keras.utils import to_categorical


y_train_one_hot = to_categorical(y_train, num_classes=4)
y_val_onehot = to_categorical(y_test, num_classes=4)

model = build_model()


history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test), verbose=2)

y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)
y_test_classes = np.argmax(y_test.values, axis=1)

accuracy = accuracy_score(y_test_classes, y_pred_classes)
f1 = f1_score(y_test_classes, y_pred_classes, average='weighted')

print(f"Accuracy: {accuracy:.4f}")
print(f"F1-Score: {f1:.4f}")

label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y.values.flatten())
y_onehot = to_categorical(y_encoded, num_classes=len(label_encoder.classes_))

from sklearn.model_selection import KFold

kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)

accuracy_list, precision_list, recall_list, f1_list = [], [], [], []

for fold, (train_idx, val_idx) in enumerate(kfold.split(X, np.argmax(y_onehot, axis=1))):
    X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]
    y_train_fold, y_val_fold = y_onehot[train_idx], y_onehot[val_idx]

    model = Sequential()
    model.add(Embedding(input_dim=5000, output_dim=30, input_length=X_train.shape[1]))
    model.add(Flatten())
    model.add(Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.01)))
    model.add(Dropout(0.5))
    model.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.01)))
    model.add(Dense(4, activation='softmax'))

    alpha_schedule = tf.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.0001, decay_steps=10000, decay_rate=0.9)
    optimizer = Adam(learning_rate=alpha_schedule)
    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

    history = model.fit(X_train_fold, y_train_fold, epochs=50, batch_size=64, validation_data=(X_val_fold, y_val_fold), verbose=0)

    metrics = model.evaluate(X_val_fold, y_val_fold, verbose=0)

    accuracy_list.append(metrics[1])
    y_pred_fold = model.predict(X_val_fold)
    y_pred_classes_fold = np.argmax(y_pred_fold, axis=1)
    y_val_classes_fold = np.argmax(y_val_fold, axis=1)

    precision_fold = precision_score(y_val_classes_fold, y_pred_classes_fold, average='weighted')
    recall_fold = recall_score(y_val_classes_fold, y_pred_classes_fold, average='weighted')
    f1_fold = f1_score(y_val_classes_fold, y_pred_classes_fold, average='weighted')

    precision_list.append(precision_fold)
    recall_list.append(recall_fold)
    f1_list.append(f1_fold)

print("Average Accuracy:", np.mean(accuracy_list))
print("Average Precision:", np.mean(precision_list))
print("Average Recall:", np.mean(recall_list))
print("Average F1 Score:", np.mean(f1_list))

X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)

dt_model = DecisionTreeClassifier(random_state=42)

param_grid = {
    'criterion': ['gini', 'entropy'],
    'splitter': ['best', 'random'],
    'max_depth': [None, 5, 10, 15],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['auto', 'sqrt', 'log2', None]
}

scorer = make_scorer(accuracy_score)

grid_search = GridSearchCV(dt_model, param_grid, scoring=scorer, cv=5)

grid_search.fit(X_train, y_train)

best_params = grid_search.best_params_
print("Best Hyperparameters:", best_params)

best_model = grid_search.best_estimator_

y_pred = best_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Test Accuracy with Best Model:", accuracy)

kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)

accuracies = []
precisions = []
recalls = []
hyperparameters = {
    'criterion':'entropy',
    'max_depth':None,
    'max_features':None,
    'min_samples_leaf':1,
    'min_samples_split':2,
    'splitter':'best'
    }

for train_index, test_index in kfold.split(X, y_encoded):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y_encoded[train_index], y_encoded[test_index]


    dt_model = DecisionTreeClassifier(random_state=42, **hyperparameters)
    dt_model.fit(X_train, y_train)

    y_pred = dt_model.predict(X_test)

    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='macro')
    recall = recall_score(y_test, y_pred, average='macro')

    accuracies.append(accuracy)
    precisions.append(precision)
    recalls.append(recall)

    print(f"Accuracy for this fold: {accuracy:.4f}")
    print(f"Precision for this fold: {precision:.4f}")
    print(f"Recall for this fold: {recall:.4f}\n")

avg_accuracy = np.mean(accuracies)
avg_precision = np.mean(precisions)
avg_recall = np.mean(recalls)

print(f"Average Accuracy: {avg_accuracy:.4f}")
print(f"Average Precision: {avg_precision:.4f}")
print(f"Average Recall: {avg_recall:.4f}")

best_model = DecisionTreeClassifier(
    criterion='entropy',
    splitter='best',
    max_depth=None,
    min_samples_split=2,
    min_samples_leaf=1,
    max_features=None,
    random_state=42
)


best_model.fit(X, y_encoded)

plt.figure(figsize=(15, 10))
plot_tree(best_model, filled=True, feature_names=X.columns, class_names=label_encoder.classes_)
plt.show()